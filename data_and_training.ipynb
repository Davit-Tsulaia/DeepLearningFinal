{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "# Imports and HyperParametres"
   ],
   "id": "9967a159784a22ae"
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-23T00:31:05.521409Z",
     "iopub.status.busy": "2026-01-23T00:31:05.520837Z",
     "iopub.status.idle": "2026-01-23T00:31:17.302374Z",
     "shell.execute_reply": "2026-01-23T00:31:17.301374Z"
    },
    "papermill": {
     "duration": 11.791801,
     "end_time": "2026-01-23T00:31:17.303876",
     "exception": false,
     "start_time": "2026-01-23T00:31:05.512075",
     "status": "completed"
    },
    "tags": [],
    "ExecuteTime": {
     "end_time": "2026-01-23T09:08:38.342135Z",
     "start_time": "2026-01-23T09:08:32.980779Z"
    }
   },
   "source": "import os\nimport re\nimport json\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.nn.utils.rnn import pad_sequence\nimport torchvision.transforms as transforms\nfrom torchvision import models\nfrom PIL import Image\nimport numpy as np\nfrom collections import Counter\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\n\ntry:\n    from nltk.translate.bleu_score import corpus_bleu, sentence_bleu, SmoothingFunction\n    import nltk\n    BLEU_AVAILABLE = True\nexcept ImportError:\n    print(\"NLTK not installed.\")\n    BLEU_AVAILABLE = False\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Device: {device}\")",
   "outputs": [],
   "execution_count": null,
   "id": "b6d573dc5c28aac5"
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-23T00:31:17.316978Z",
     "iopub.status.busy": "2026-01-23T00:31:17.316170Z",
     "iopub.status.idle": "2026-01-23T00:31:17.321851Z",
     "shell.execute_reply": "2026-01-23T00:31:17.321310Z"
    },
    "papermill": {
     "duration": 0.013454,
     "end_time": "2026-01-23T00:31:17.323250",
     "exception": false,
     "start_time": "2026-01-23T00:31:17.309796",
     "status": "completed"
    },
    "tags": [],
    "ExecuteTime": {
     "end_time": "2026-01-23T09:08:39.032103Z",
     "start_time": "2026-01-23T09:08:38.796643Z"
    }
   },
   "source": [
    "import os\n",
    "\n",
    "IS_KAGGLE = os.path.exists('/kaggle/input')\n",
    "\n",
    "if IS_KAGGLE:\n",
    "    DATA_DIR = ('/kaggle/input/deeplearn/caption_data')\n",
    "    IMAGES_DIR = f'{DATA_DIR}/Images'\n",
    "    CAPTIONS_FILE = f'{DATA_DIR}/captions.txt'\n",
    "    SAVE_DIR = '/kaggle/working'\n",
    "else:\n",
    "    DATA_DIR = './caption_data'\n",
    "    IMAGES_DIR = f'{DATA_DIR}/Images'\n",
    "    CAPTIONS_FILE = f'{DATA_DIR}/captions.txt'\n",
    "    SAVE_DIR = '.'\n",
    "\n",
    "EMBED_DIM = 256\n",
    "HIDDEN_DIM = 768\n",
    "ATTENTION_DIM = 512\n",
    "ENCODER_DIM = 2048\n",
    "DROPOUT = 0.5\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "NUM_EPOCHS = 17\n",
    "LEARNING_RATE = 3e-4\n",
    "ENCODER_LR = 1e-4\n",
    "FINE_TUNE_ENCODER = True\n",
    "FINE_TUNE_AFTER = 5\n",
    "\n",
    "MIN_WORD_FREQ = 5\n",
    "MAX_CAPTION_LEN = 50\n",
    "\n",
    "TRAIN_SIZE = 6000\n",
    "VAL_SIZE = 1000\n",
    "TEST_SIZE = 1000\n",
    "\n",
    "print(f\"Save directory: {SAVE_DIR}\")\n",
    "print(f\"Data directory: {DATA_DIR}\")"
   ],
   "outputs": [],
   "execution_count": null,
   "id": "1310ca577373fa1a"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-23T09:08:39.087754501Z",
     "start_time": "2026-01-22T12:12:21.666228Z"
    },
    "execution": {
     "iopub.execute_input": "2026-01-23T00:31:17.366979Z",
     "iopub.status.busy": "2026-01-23T00:31:17.366368Z",
     "iopub.status.idle": "2026-01-23T00:31:17.380465Z",
     "shell.execute_reply": "2026-01-23T00:31:17.379903Z"
    },
    "papermill": {
     "duration": 0.021844,
     "end_time": "2026-01-23T00:31:17.381889",
     "exception": false,
     "start_time": "2026-01-23T00:31:17.360045",
     "status": "completed"
    },
    "tags": []
   },
   "source": "## Define Classes and Functions\n",
   "id": "66e2ffee429df952"
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-23T00:31:17.406208Z",
     "iopub.status.busy": "2026-01-23T00:31:17.405929Z",
     "iopub.status.idle": "2026-01-23T00:31:17.415395Z",
     "shell.execute_reply": "2026-01-23T00:31:17.414782Z"
    },
    "papermill": {
     "duration": 0.017468,
     "end_time": "2026-01-23T00:31:17.416798",
     "exception": false,
     "start_time": "2026-01-23T00:31:17.399330",
     "status": "completed"
    },
    "tags": [],
    "ExecuteTime": {
     "end_time": "2026-01-23T17:09:47.439669Z",
     "start_time": "2026-01-23T17:09:47.410177Z"
    }
   },
   "source": "def clean_caption(text):\n    \"\"\"Clean caption: lowercase, remove punctuation/numbers/single chars.\"\"\"\n    text = text.lower()\n    text = re.sub(r'[^\\w\\s]', '', text)\n    text = re.sub(r'\\d+', '', text)\n    words = [w for w in text.split() if len(w) > 1 or w in ['a', 'i']]\n    return ' '.join(words).strip()\n\nclass Vocabulary:\n    \"\"\"Word-to-index mappings with special tokens: PAD, SOS, EOS, UNK.\"\"\"\n    \n    def __init__(self, min_freq=5):\n        self.min_freq = min_freq\n        self.word2idx = {}\n        self.idx2word = {}\n        self.word_freq = Counter()\n        self.PAD_TOKEN = '<PAD>'\n        self.SOS_TOKEN = '<SOS>'\n        self.EOS_TOKEN = '<EOS>'\n        self.UNK_TOKEN = '<UNK>'\n        self._init_special_tokens()\n    \n    def _init_special_tokens(self):\n        for idx, token in enumerate([self.PAD_TOKEN, self.SOS_TOKEN, self.EOS_TOKEN, self.UNK_TOKEN]):\n            self.word2idx[token] = idx\n            self.idx2word[idx] = token\n    \n    def build_vocabulary(self, captions):\n        for caption in tqdm(captions, desc=\"Building vocab\"):\n            self.word_freq.update(caption.split())\n        \n        idx = len(self.word2idx)\n        for word, freq in self.word_freq.items():\n            if freq >= self.min_freq and word not in self.word2idx:\n                self.word2idx[word] = idx\n                self.idx2word[idx] = word\n                idx += 1\n        \n        print(f\"Vocabulary: {len(self.word2idx)} words (filtered {sum(1 for f in self.word_freq.values() if f < self.min_freq)} rare words)\")\n    \n    def encode(self, caption):\n        caption = clean_caption(caption)\n        indices = [self.word2idx[self.SOS_TOKEN]]\n        for token in caption.split():\n            indices.append(self.word2idx.get(token, self.word2idx[self.UNK_TOKEN]))\n        indices.append(self.word2idx[self.EOS_TOKEN])\n        return indices\n    \n    def decode(self, indices):\n        words = []\n        for idx in indices:\n            if isinstance(idx, torch.Tensor):\n                idx = idx.item()\n            word = self.idx2word.get(idx, self.UNK_TOKEN)\n            if word == self.EOS_TOKEN:\n                break\n            if word not in [self.PAD_TOKEN, self.SOS_TOKEN]:\n                words.append(word)\n        return ' '.join(words)\n    \n    def __len__(self):\n        return len(self.word2idx)\n    \n    def save(self, path):\n        data = {'word2idx': self.word2idx, 'idx2word': {int(k): v for k, v in self.idx2word.items()}, \n                'min_freq': self.min_freq, 'word_freq': dict(self.word_freq)}\n        with open(path, 'w') as f:\n            json.dump(data, f)\n    \n    @classmethod\n    def load(cls, path):\n        with open(path, 'r') as f:\n            data = json.load(f)\n        vocab = cls(min_freq=data['min_freq'])\n        vocab.word2idx = data['word2idx']\n        vocab.idx2word = {int(k): v for k, v in data['idx2word'].items()}\n        if 'word_freq' in data:\n            vocab.word_freq = Counter(data['word_freq'])\n        return vocab",
   "outputs": [],
   "execution_count": 3,
   "id": "c5ec16d3deadfd06"
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-23T00:31:17.429608Z",
     "iopub.status.busy": "2026-01-23T00:31:17.429377Z",
     "iopub.status.idle": "2026-01-23T00:31:17.436520Z",
     "shell.execute_reply": "2026-01-23T00:31:17.435903Z"
    },
    "papermill": {
     "duration": 0.014996,
     "end_time": "2026-01-23T00:31:17.437858",
     "exception": false,
     "start_time": "2026-01-23T00:31:17.422862",
     "status": "completed"
    },
    "tags": [],
    "ExecuteTime": {
     "end_time": "2026-01-23T17:09:47.483665Z",
     "start_time": "2026-01-23T17:09:47.472381Z"
    }
   },
   "source": "def load_captions(captions_file):\n    \"\"\"Load and clean captions from file.\"\"\"\n    image_captions = {}\n    \n    with open(captions_file, 'r', encoding='utf-8') as f:\n        lines = f.readlines()\n    \n    start_idx = 1 if 'image' in lines[0].lower() and 'caption' in lines[0].lower() else 0\n    \n    for line in lines[start_idx:]:\n        line = line.strip()\n        if not line:\n            continue\n        \n        parts = line.split(',', 1) if ',' in line else line.split('\\t', 1)\n        if len(parts) != 2:\n            continue\n        \n        image_name, caption = parts[0].strip(), parts[1].strip()\n        if '#' in image_name:\n            image_name = image_name.split('#')[0]\n        \n        cleaned = clean_caption(caption)\n        if cleaned:\n            if image_name not in image_captions:\n                image_captions[image_name] = []\n            image_captions[image_name].append(cleaned)\n    \n    print(f\"Loaded {len(image_captions)} images, {sum(len(c) for c in image_captions.values())} captions\")\n    return image_captions\n\ndef create_data_splits(image_captions, train_ratio=0.75, seed=42):\n    \"\"\"Split data into train/val/test sets.\"\"\"\n    image_names = list(image_captions.keys())\n    np.random.seed(seed)\n    np.random.shuffle(image_names)\n    \n    n_total = len(image_names)\n    n_train = int(n_total * train_ratio)\n    n_val = int(n_total * 0.1)\n    \n    train_captions = {img: image_captions[img] for img in image_names[:n_train]}\n    val_captions = {img: image_captions[img] for img in image_names[n_train:n_train + n_val]}\n    test_captions = {img: image_captions[img] for img in image_names[n_train + n_val:]}\n    \n    print(f\"Split: {len(train_captions)} train, {len(val_captions)} val, {len(test_captions)} test\")\n    return train_captions, val_captions, test_captions",
   "outputs": [],
   "execution_count": 4,
   "id": "aacaa711d9e94e45"
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-23T00:31:17.450537Z",
     "iopub.status.busy": "2026-01-23T00:31:17.450090Z",
     "iopub.status.idle": "2026-01-23T00:31:17.455091Z",
     "shell.execute_reply": "2026-01-23T00:31:17.454448Z"
    },
    "papermill": {
     "duration": 0.012914,
     "end_time": "2026-01-23T00:31:17.456516",
     "exception": false,
     "start_time": "2026-01-23T00:31:17.443602",
     "status": "completed"
    },
    "tags": [],
    "ExecuteTime": {
     "end_time": "2026-01-23T17:09:47.581129Z",
     "start_time": "2026-01-23T17:09:47.554618Z"
    }
   },
   "source": "class CaptionDataset(Dataset):\n    \"\"\"Dataset for image-caption pairs.\"\"\"\n    \n    def __init__(self, image_captions, images_dir, vocab, transform=None, max_len=50):\n        self.images_dir = images_dir\n        self.vocab = vocab\n        self.transform = transform\n        self.max_len = max_len\n        self.samples = [(img, cap) for img, caps in image_captions.items() for cap in caps]\n    \n    def __len__(self):\n        return len(self.samples)\n    \n    def __getitem__(self, idx):\n        img_name, caption = self.samples[idx]\n        image = Image.open(os.path.join(self.images_dir, img_name)).convert('RGB')\n        if self.transform:\n            image = self.transform(image)\n        \n        encoded = self.vocab.encode(caption)\n        if len(encoded) > self.max_len:\n            encoded = encoded[:self.max_len-1] + [self.vocab.word2idx[self.vocab.EOS_TOKEN]]\n        \n        return image, torch.tensor(encoded, dtype=torch.long)\n\ndef collate_fn(batch):\n    \"\"\"Pad captions to same length within batch.\"\"\"\n    images, captions = zip(*batch)\n    images = torch.stack(images, dim=0)\n    lengths = torch.tensor([len(c) for c in captions], dtype=torch.long)\n    captions = pad_sequence(captions, batch_first=True, padding_value=0)\n    return images, captions, lengths\n\ntrain_transform = transforms.Compose([\n    transforms.Resize((256, 256)),\n    transforms.RandomCrop(224),\n    transforms.RandomHorizontalFlip(),\n    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\nval_transform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])",
   "outputs": [],
   "execution_count": 5,
   "id": "4978bddf081f540a"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-23T09:08:39.510818997Z",
     "start_time": "2026-01-22T12:12:22.001673Z"
    },
    "execution": {
     "iopub.execute_input": "2026-01-23T00:31:17.469392Z",
     "iopub.status.busy": "2026-01-23T00:31:17.469191Z",
     "iopub.status.idle": "2026-01-23T00:31:17.474740Z",
     "shell.execute_reply": "2026-01-23T00:31:17.474054Z"
    },
    "papermill": {
     "duration": 0.013616,
     "end_time": "2026-01-23T00:31:17.476113",
     "exception": false,
     "start_time": "2026-01-23T00:31:17.462497",
     "status": "completed"
    },
    "tags": []
   },
   "source": "## Data Processing\n\nLoading captions, building vocabulary, and splitting dataset.",
   "id": "8a748d8b91e9531e"
  },
  {
   "cell_type": "code",
   "metadata": {
    "papermill": {
     "duration": 0.005492,
     "end_time": "2026-01-23T00:31:17.487381",
     "exception": false,
     "start_time": "2026-01-23T00:31:17.481889",
     "status": "completed"
    },
    "tags": []
   },
   "source": "if not os.path.exists(CAPTIONS_FILE):\n    print(f\"WARNING: Captions file not found at {CAPTIONS_FILE}\")\nelse:\n    image_captions = load_captions(CAPTIONS_FILE)\n    \n    train_captions, val_captions, test_captions = create_data_splits(image_captions, 0.75)\n    \n    all_train_captions = [cap for caps in train_captions.values() for cap in caps]\n    vocab = Vocabulary(min_freq=MIN_WORD_FREQ)\n    vocab.build_vocabulary(all_train_captions)\n    \n    vocab.save(os.path.join(SAVE_DIR, 'vocab.json'))",
   "outputs": [],
   "execution_count": null,
   "id": "b0e2ea70e0656e0b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-23T09:08:39.541163752Z",
     "start_time": "2026-01-22T12:12:22.084765Z"
    },
    "execution": {
     "iopub.execute_input": "2026-01-23T00:31:17.499945Z",
     "iopub.status.busy": "2026-01-23T00:31:17.499459Z",
     "iopub.status.idle": "2026-01-23T00:31:17.922257Z",
     "shell.execute_reply": "2026-01-23T00:31:17.921378Z"
    },
    "papermill": {
     "duration": 0.430886,
     "end_time": "2026-01-23T00:31:17.923802",
     "exception": false,
     "start_time": "2026-01-23T00:31:17.492916",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": "if os.path.exists(CAPTIONS_FILE):\n    train_dataset = CaptionDataset(\n        train_captions, IMAGES_DIR, vocab, \n        transform=train_transform, max_len=MAX_CAPTION_LEN\n    )\n    \n    val_dataset = CaptionDataset(\n        val_captions, IMAGES_DIR, vocab,\n        transform=val_transform, max_len=MAX_CAPTION_LEN\n    )\n    \n    test_dataset = CaptionDataset(\n        test_captions, IMAGES_DIR, vocab,\n        transform=val_transform, max_len=MAX_CAPTION_LEN\n    )\n    \n    train_loader = DataLoader(\n        train_dataset, batch_size=BATCH_SIZE, shuffle=True,\n        collate_fn=collate_fn, num_workers=4, pin_memory=True\n    )\n    \n    val_loader = DataLoader(\n        val_dataset, batch_size=BATCH_SIZE, shuffle=False,\n        collate_fn=collate_fn, num_workers=4, pin_memory=True\n    )\n    \n    test_loader = DataLoader(\n        test_dataset, batch_size=BATCH_SIZE, shuffle=False,\n        collate_fn=collate_fn, num_workers=4, pin_memory=True\n    )\n    \n    print(f\"Dataloaders: {len(train_loader)} train, {len(val_loader)} val, {len(test_loader)} test batches\")",
   "id": "14bea55968e7ea5f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-23T09:08:39.552611653Z",
     "start_time": "2026-01-22T12:12:22.516754Z"
    },
    "execution": {
     "iopub.execute_input": "2026-01-23T00:31:17.937505Z",
     "iopub.status.busy": "2026-01-23T00:31:17.937241Z",
     "iopub.status.idle": "2026-01-23T00:31:19.499512Z",
     "shell.execute_reply": "2026-01-23T00:31:19.498738Z"
    },
    "papermill": {
     "duration": 1.573511,
     "end_time": "2026-01-23T00:31:19.503433",
     "exception": false,
     "start_time": "2026-01-23T00:31:17.929922",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": "if os.path.exists(CAPTIONS_FILE):\n    caption_lengths = [len(cap.split()) for cap in all_train_captions]\n    \n    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n    \n    axes[0, 0].hist(caption_lengths, bins=30, edgecolor='black', alpha=0.7, color='steelblue')\n    axes[0, 0].axvline(np.mean(caption_lengths), color='red', linestyle='--', \n                       label=f'Mean: {np.mean(caption_lengths):.1f}')\n    axes[0, 0].axvline(np.median(caption_lengths), color='green', linestyle='--',\n                       label=f'Median: {np.median(caption_lengths):.1f}')\n    axes[0, 0].set_xlabel('Caption Length (words)')\n    axes[0, 0].set_ylabel('Frequency')\n    axes[0, 0].set_title('Caption Length Distribution')\n    axes[0, 0].legend()\n    axes[0, 0].grid(True, alpha=0.3)\n    \n    top_words = vocab.word_freq.most_common(30)\n    words, freqs = zip(*top_words)\n    axes[0, 1].barh(range(len(words)), freqs, color='coral')\n    axes[0, 1].set_yticks(range(len(words)))\n    axes[0, 1].set_yticklabels(words)\n    axes[0, 1].invert_yaxis()\n    axes[0, 1].set_xlabel('Frequency')\n    axes[0, 1].set_title('Top 30 Most Common Words')\n    axes[0, 1].grid(True, alpha=0.3, axis='x')\n    \n    thresholds = [1, 2, 3, 5, 10, 15, 20]\n    vocab_sizes = []\n    for thresh in thresholds:\n        size = sum(1 for freq in vocab.word_freq.values() if freq >= thresh)\n        vocab_sizes.append(size)\n    \n    axes[1, 0].plot(thresholds, vocab_sizes, marker='o', linewidth=2, color='purple')\n    axes[1, 0].axvline(MIN_WORD_FREQ, color='red', linestyle='--', \n                       label=f'Selected threshold: {MIN_WORD_FREQ}')\n    axes[1, 0].set_xlabel('Minimum Word Frequency')\n    axes[1, 0].set_ylabel('Vocabulary Size')\n    axes[1, 0].set_title('Vocabulary Size vs. Frequency Threshold')\n    axes[1, 0].legend()\n    axes[1, 0].grid(True, alpha=0.3)\n    \n    split_names = ['Train', 'Validation', 'Test']\n    split_sizes = [len(train_captions), len(val_captions), len(test_captions)]\n    colors = ['#2ecc71', '#3498db', '#e74c3c']\n    \n    axes[1, 1].pie(split_sizes, labels=split_names, autopct='%1.1f%%', \n                   colors=colors, explode=(0.02, 0.02, 0.02))\n    axes[1, 1].set_title('Data Split Distribution')\n    \n    plt.tight_layout()\n    plt.savefig(os.path.join(SAVE_DIR, 'data_exploration.png'), dpi=150)\n    plt.show()\n    \n    print(f\"Caption stats: min {min(caption_lengths)}, max {max(caption_lengths)}, mean {np.mean(caption_lengths):.1f}\")",
   "id": "6ed8e72e4cf68d78"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.008519,
     "end_time": "2026-01-23T00:31:22.238789",
     "exception": false,
     "start_time": "2026-01-23T00:31:22.230270",
     "status": "completed"
    },
    "tags": []
   },
   "source": "## Model Architecture\n\nCNN encoder using ResNet-50 for feature extraction, attention mechanism for focusing on relevant image regions, and LSTM decoder for caption generation.",
   "id": "9cfb30672341867e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-23T09:08:39.558678247Z",
     "start_time": "2026-01-22T12:12:31.354966Z"
    },
    "execution": {
     "iopub.execute_input": "2026-01-23T00:31:22.306551Z",
     "iopub.status.busy": "2026-01-23T00:31:22.306160Z",
     "iopub.status.idle": "2026-01-23T00:31:22.309925Z",
     "shell.execute_reply": "2026-01-23T00:31:22.309143Z"
    },
    "papermill": {
     "duration": 0.01472,
     "end_time": "2026-01-23T00:31:22.311396",
     "exception": false,
     "start_time": "2026-01-23T00:31:22.296676",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": "class EncoderCNN(nn.Module):\n    def __init__(self, encoded_image_size=7, fine_tune=False):\n        super(EncoderCNN, self).__init__()\n        \n        try:\n            resnet = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1)\n        except:\n            resnet = models.resnet50(weights=None)\n            print(\"Warning: ResNet loaded without pretrained weights\")\n        \n        self.resnet = nn.Sequential(*list(resnet.children())[:-2])\n        self.adaptive_pool = nn.AdaptiveAvgPool2d((encoded_image_size, encoded_image_size))\n        \n        self.set_fine_tuning(fine_tune)\n    \n    def set_fine_tuning(self, fine_tune):\n        for param in self.resnet.parameters():\n            param.requires_grad = fine_tune\n    \n    def forward(self, images):\n        features = self.resnet(images)\n        features = self.adaptive_pool(features)\n        \n        batch_size = features.size(0)\n        features = features.permute(0, 2, 3, 1)\n        features = features.view(batch_size, -1, 2048)\n        \n        return features",
   "id": "d49eecd9aa93b21"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-23T09:08:39.559483995Z",
     "start_time": "2026-01-22T12:12:31.797441Z"
    },
    "execution": {
     "iopub.execute_input": "2026-01-23T00:31:22.329715Z",
     "iopub.status.busy": "2026-01-23T00:31:22.329506Z",
     "iopub.status.idle": "2026-01-23T00:31:22.335830Z",
     "shell.execute_reply": "2026-01-23T00:31:22.335069Z"
    },
    "papermill": {
     "duration": 0.01738,
     "end_time": "2026-01-23T00:31:22.337256",
     "exception": false,
     "start_time": "2026-01-23T00:31:22.319876",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    \"\"\"Soft attention mechanism.\"\"\"\n",
    "    \n",
    "    def __init__(self, encoder_dim, decoder_dim, attention_dim):\n",
    "        super(Attention, self).__init__()\n",
    "        self.encoder_att = nn.Linear(encoder_dim, attention_dim)\n",
    "        self.decoder_att = nn.Linear(decoder_dim, attention_dim)\n",
    "        self.full_att = nn.Linear(attention_dim, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "    \n",
    "    def forward(self, encoder_out, decoder_hidden):\n",
    "        att1 = self.encoder_att(encoder_out)\n",
    "        att2 = self.decoder_att(decoder_hidden)\n",
    "        att = self.full_att(self.relu(att1 + att2.unsqueeze(1))).squeeze(2)\n",
    "        alpha = self.softmax(att)\n",
    "        context = (encoder_out * alpha.unsqueeze(2)).sum(dim=1)\n",
    "        return context, alpha"
   ],
   "id": "b996a86d4796b76c"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-23T09:08:39.561581909Z",
     "start_time": "2026-01-22T12:12:31.934360Z"
    },
    "execution": {
     "iopub.execute_input": "2026-01-23T00:31:22.393114Z",
     "iopub.status.busy": "2026-01-23T00:31:22.392846Z",
     "iopub.status.idle": "2026-01-23T00:31:22.415646Z",
     "shell.execute_reply": "2026-01-23T00:31:22.414927Z"
    },
    "papermill": {
     "duration": 0.033606,
     "end_time": "2026-01-23T00:31:22.417170",
     "exception": false,
     "start_time": "2026-01-23T00:31:22.383564",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DecoderLSTM(nn.Module):\n",
    "    \"\"\"\n",
    "    LSTM Decoder with Attention for image captioning.\n",
    "    \n",
    "    Generates captions word by word, attending to different\n",
    "    parts of the encoded image at each timestep.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, embed_dim, decoder_dim, attention_dim, vocab_size, \n",
    "                 encoder_dim=2048, dropout=0.5):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            embed_dim: Dimension of word embeddings\n",
    "            decoder_dim: Dimension of LSTM hidden state\n",
    "            attention_dim: Dimension of attention network\n",
    "            vocab_size: Size of vocabulary\n",
    "            encoder_dim: Dimension of encoded image features\n",
    "            dropout: Dropout probability\n",
    "        \"\"\"\n",
    "        super(DecoderLSTM, self).__init__()\n",
    "        \n",
    "        self.encoder_dim = encoder_dim\n",
    "        self.decoder_dim = decoder_dim\n",
    "        self.attention_dim = attention_dim\n",
    "        self.embed_dim = embed_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        # Attention network\n",
    "        self.attention = Attention(encoder_dim, decoder_dim, attention_dim)\n",
    "        \n",
    "        # Word embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        \n",
    "        # Dropout layer\n",
    "        self.dropout_layer = nn.Dropout(dropout)\n",
    "        \n",
    "        # LSTM cell (not full LSTM - we need step-by-step control)\n",
    "        # Input: concatenation of embedding and attention-weighted encoding\n",
    "        self.lstm_cell = nn.LSTMCell(embed_dim + encoder_dim, decoder_dim)\n",
    "        \n",
    "        # Linear layers to initialize LSTM states from encoder output\n",
    "        self.init_h = nn.Linear(encoder_dim, decoder_dim)\n",
    "        self.init_c = nn.Linear(encoder_dim, decoder_dim)\n",
    "        \n",
    "        # Linear layer to create a sigmoid-activated gate\n",
    "        self.f_beta = nn.Linear(decoder_dim, encoder_dim)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        # Linear layer to find scores over vocabulary\n",
    "        self.fc = nn.Linear(decoder_dim, vocab_size)\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        \"\"\"Initialize some weights with uniform distribution.\"\"\"\n",
    "        self.embedding.weight.data.uniform_(-0.1, 0.1)\n",
    "        self.fc.bias.data.fill_(0)\n",
    "        self.fc.weight.data.uniform_(-0.1, 0.1)\n",
    "    \n",
    "    def init_hidden_state(self, encoder_out):\n",
    "        \"\"\"\n",
    "        Initialize LSTM hidden state from encoder output.\n",
    "        \n",
    "        Args:\n",
    "            encoder_out: (batch_size, num_pixels, encoder_dim)\n",
    "        \n",
    "        Returns:\n",
    "            h: (batch_size, decoder_dim)\n",
    "            c: (batch_size, decoder_dim)\n",
    "        \"\"\"\n",
    "        # Mean of encoder output across spatial dimensions\n",
    "        mean_encoder_out = encoder_out.mean(dim=1)  # (batch, encoder_dim)\n",
    "        \n",
    "        h = self.init_h(mean_encoder_out)  # (batch, decoder_dim)\n",
    "        c = self.init_c(mean_encoder_out)  # (batch, decoder_dim)\n",
    "        \n",
    "        return h, c\n",
    "    \n",
    "    def forward(self, encoder_out, captions, caption_lengths):\n",
    "        \"\"\"\n",
    "        Forward pass for training with teacher forcing.\n",
    "        \n",
    "        Args:\n",
    "            encoder_out: (batch_size, num_pixels, encoder_dim)\n",
    "            captions: (batch_size, max_caption_length)\n",
    "            caption_lengths: (batch_size,)\n",
    "        \n",
    "        Returns:\n",
    "            predictions: (batch_size, max_caption_length, vocab_size)\n",
    "            alphas: (batch_size, max_caption_length, num_pixels)\n",
    "        \"\"\"\n",
    "        batch_size = encoder_out.size(0)\n",
    "        num_pixels = encoder_out.size(1)\n",
    "        \n",
    "        # Sort by decreasing caption length for efficient packing\n",
    "        caption_lengths, sort_idx = caption_lengths.sort(dim=0, descending=True)\n",
    "        encoder_out = encoder_out[sort_idx]\n",
    "        captions = captions[sort_idx]\n",
    "        \n",
    "        # Embed captions\n",
    "        embeddings = self.embedding(captions)  # (batch, max_len, embed_dim)\n",
    "        \n",
    "        # Initialize LSTM state\n",
    "        h, c = self.init_hidden_state(encoder_out)\n",
    "        \n",
    "        # We won't decode at <EOS> position, since we've finished when we hit <EOS>\n",
    "        # So decode_length = caption_length - 1\n",
    "        decode_lengths = (caption_lengths - 1).tolist()\n",
    "        max_decode_length = max(decode_lengths)\n",
    "        \n",
    "        # Create tensors to hold predictions and attention weights\n",
    "        predictions = torch.zeros(batch_size, max_decode_length, self.vocab_size).to(encoder_out.device)\n",
    "        alphas = torch.zeros(batch_size, max_decode_length, num_pixels).to(encoder_out.device)\n",
    "        \n",
    "        # For each timestep\n",
    "        for t in range(max_decode_length):\n",
    "            # Determine batch size at this timestep (some sequences may have ended)\n",
    "            batch_size_t = sum([l > t for l in decode_lengths])\n",
    "            \n",
    "            # Attention\n",
    "            attention_weighted_encoding, alpha = self.attention(\n",
    "                encoder_out[:batch_size_t], \n",
    "                h[:batch_size_t]\n",
    "            )\n",
    "            \n",
    "            # Gating scalar (for doubly stochastic attention)\n",
    "            gate = self.sigmoid(self.f_beta(h[:batch_size_t]))\n",
    "            attention_weighted_encoding = gate * attention_weighted_encoding\n",
    "            \n",
    "            # LSTM input: concatenate embedding and attention-weighted encoding\n",
    "            lstm_input = torch.cat([\n",
    "                embeddings[:batch_size_t, t, :], \n",
    "                attention_weighted_encoding\n",
    "            ], dim=1)\n",
    "            \n",
    "            # LSTM step\n",
    "            h_new, c_new = self.lstm_cell(lstm_input, (h[:batch_size_t], c[:batch_size_t]))\n",
    "            \n",
    "            # Update hidden states\n",
    "            h = h.clone()\n",
    "            c = c.clone()\n",
    "            h[:batch_size_t] = h_new\n",
    "            c[:batch_size_t] = c_new\n",
    "            \n",
    "            # Predict next word\n",
    "            preds = self.fc(self.dropout_layer(h_new))  # (batch_size_t, vocab_size)\n",
    "            \n",
    "            # Store predictions and attention weights\n",
    "            predictions[:batch_size_t, t, :] = preds\n",
    "            alphas[:batch_size_t, t, :] = alpha\n",
    "        \n",
    "        return predictions, alphas, captions, decode_lengths, sort_idx\n",
    "    \n",
    "    def generate(self, encoder_out, vocab, max_len=50, beam_size=1):\n",
    "        \"\"\"\n",
    "        Generate caption for a single image using greedy decoding or beam search.\n",
    "        \n",
    "        Args:\n",
    "            encoder_out: (1, num_pixels, encoder_dim)\n",
    "            vocab: Vocabulary object\n",
    "            max_len: Maximum caption length\n",
    "            beam_size: Beam size for beam search (1 = greedy)\n",
    "        \n",
    "        Returns:\n",
    "            caption: Generated caption string\n",
    "            attention_weights: List of attention weights\n",
    "        \"\"\"\n",
    "        if beam_size == 1:\n",
    "            return self._greedy_decode(encoder_out, vocab, max_len)\n",
    "        else:\n",
    "            return self._beam_search(encoder_out, vocab, max_len, beam_size)\n",
    "    \n",
    "    def _greedy_decode(self, encoder_out, vocab, max_len):\n",
    "        \"\"\"Greedy decoding - select most probable word at each step.\"\"\"\n",
    "        device = encoder_out.device\n",
    "        \n",
    "        # Initialize LSTM state\n",
    "        h, c = self.init_hidden_state(encoder_out)\n",
    "        \n",
    "        # Start with <SOS> token\n",
    "        word_idx = vocab.word2idx[vocab.SOS_TOKEN]\n",
    "        \n",
    "        caption = []\n",
    "        attention_weights = []\n",
    "        \n",
    "        for _ in range(max_len):\n",
    "            # Embed current word\n",
    "            embedding = self.embedding(torch.tensor([word_idx]).to(device))  # (1, embed_dim)\n",
    "            \n",
    "            # Attention\n",
    "            attention_weighted_encoding, alpha = self.attention(encoder_out, h)\n",
    "            attention_weights.append(alpha.squeeze(0).cpu().detach().numpy())\n",
    "            \n",
    "            # Gating\n",
    "            gate = self.sigmoid(self.f_beta(h))\n",
    "            attention_weighted_encoding = gate * attention_weighted_encoding\n",
    "            \n",
    "            # LSTM step\n",
    "            lstm_input = torch.cat([embedding, attention_weighted_encoding], dim=1)\n",
    "            h, c = self.lstm_cell(lstm_input, (h, c))\n",
    "            \n",
    "            # Predict\n",
    "            scores = self.fc(h)\n",
    "            word_idx = scores.argmax(dim=1).item()\n",
    "            \n",
    "            # Check for <EOS>\n",
    "            if word_idx == vocab.word2idx[vocab.EOS_TOKEN]:\n",
    "                break\n",
    "            \n",
    "            caption.append(vocab.idx2word[word_idx])\n",
    "        \n",
    "        return ' '.join(caption), attention_weights\n",
    "    \n",
    "    def _beam_search(self, encoder_out, vocab, max_len, beam_size):\n",
    "        \"\"\"Beam search decoding for better results.\"\"\"\n",
    "        device = encoder_out.device\n",
    "        num_pixels = encoder_out.size(1)\n",
    "        \n",
    "        # Expand encoder output for beam search\n",
    "        encoder_out = encoder_out.expand(beam_size, num_pixels, self.encoder_dim)\n",
    "        \n",
    "        # Initialize\n",
    "        k_prev_words = torch.tensor([[vocab.word2idx[vocab.SOS_TOKEN]]] * beam_size).to(device)\n",
    "        seqs = k_prev_words\n",
    "        top_k_scores = torch.zeros(beam_size, 1).to(device)\n",
    "        \n",
    "        # Initialize LSTM states\n",
    "        h, c = self.init_hidden_state(encoder_out)\n",
    "        \n",
    "        complete_seqs = []\n",
    "        complete_seqs_scores = []\n",
    "        \n",
    "        for step in range(max_len):\n",
    "            embeddings = self.embedding(k_prev_words).squeeze(1)\n",
    "            attention_weighted_encoding, _ = self.attention(encoder_out, h)\n",
    "            gate = self.sigmoid(self.f_beta(h))\n",
    "            attention_weighted_encoding = gate * attention_weighted_encoding\n",
    "            \n",
    "            lstm_input = torch.cat([embeddings, attention_weighted_encoding], dim=1)\n",
    "            h, c = self.lstm_cell(lstm_input, (h, c))\n",
    "            \n",
    "            scores = self.fc(h)\n",
    "            scores = torch.log_softmax(scores, dim=1)\n",
    "            scores = top_k_scores.expand_as(scores) + scores\n",
    "            \n",
    "            if step == 0:\n",
    "                top_k_scores, top_k_words = scores[0].topk(beam_size, 0, True, True)\n",
    "            else:\n",
    "                top_k_scores, top_k_words = scores.view(-1).topk(beam_size, 0, True, True)\n",
    "            \n",
    "            prev_word_inds = top_k_words // self.vocab_size\n",
    "            next_word_inds = top_k_words % self.vocab_size\n",
    "            \n",
    "            seqs = torch.cat([seqs[prev_word_inds], next_word_inds.unsqueeze(1)], dim=1)\n",
    "            \n",
    "            incomplete_inds = [ind for ind, word in enumerate(next_word_inds) \n",
    "                             if word != vocab.word2idx[vocab.EOS_TOKEN]]\n",
    "            complete_inds = [ind for ind, word in enumerate(next_word_inds) \n",
    "                           if word == vocab.word2idx[vocab.EOS_TOKEN]]\n",
    "            \n",
    "            if len(complete_inds) > 0:\n",
    "                complete_seqs.extend(seqs[complete_inds].tolist())\n",
    "                complete_seqs_scores.extend(top_k_scores[complete_inds].tolist())\n",
    "            \n",
    "            beam_size = len(incomplete_inds)\n",
    "            if beam_size == 0:\n",
    "                break\n",
    "            \n",
    "            seqs = seqs[incomplete_inds]\n",
    "            h = h[prev_word_inds[incomplete_inds]]\n",
    "            c = c[prev_word_inds[incomplete_inds]]\n",
    "            encoder_out = encoder_out[prev_word_inds[incomplete_inds]]\n",
    "            top_k_scores = top_k_scores[incomplete_inds].unsqueeze(1)\n",
    "            k_prev_words = next_word_inds[incomplete_inds].unsqueeze(1)\n",
    "        \n",
    "        if len(complete_seqs) == 0:\n",
    "            complete_seqs = seqs.tolist()\n",
    "            complete_seqs_scores = top_k_scores.squeeze(1).tolist()\n",
    "        \n",
    "        best_seq_idx = complete_seqs_scores.index(max(complete_seqs_scores))\n",
    "        best_seq = complete_seqs[best_seq_idx]\n",
    "        \n",
    "        caption = [vocab.idx2word[idx] for idx in best_seq[1:] \n",
    "                  if idx not in [vocab.word2idx[vocab.SOS_TOKEN], \n",
    "                                vocab.word2idx[vocab.EOS_TOKEN],\n",
    "                                vocab.word2idx[vocab.PAD_TOKEN]]]\n",
    "        \n",
    "        return ' '.join(caption), []"
   ],
   "id": "24273695139cc55d"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.008394,
     "end_time": "2026-01-23T00:31:23.030110",
     "exception": false,
     "start_time": "2026-01-23T00:31:23.021716",
     "status": "completed"
    },
    "tags": []
   },
   "source": "## Model Training\n\nTraining with cross-entropy loss and attention regularization. Two-stage training approach: encoder frozen initially, then fine-tuned after 5 epochs.",
   "id": "312eae9be5c87da0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-23T09:08:39.562793054Z",
     "start_time": "2026-01-22T12:12:32.250106Z"
    },
    "execution": {
     "iopub.execute_input": "2026-01-23T00:31:23.048158Z",
     "iopub.status.busy": "2026-01-23T00:31:23.047879Z",
     "iopub.status.idle": "2026-01-23T00:31:23.061038Z",
     "shell.execute_reply": "2026-01-23T00:31:23.060402Z"
    },
    "papermill": {
     "duration": 0.024181,
     "end_time": "2026-01-23T00:31:23.062601",
     "exception": false,
     "start_time": "2026-01-23T00:31:23.038420",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": "def train_one_epoch(encoder, decoder, train_loader, criterion, \n                    decoder_optimizer, encoder_optimizer, device, alpha_c=1.0):\n    encoder.train()\n    decoder.train()\n    \n    total_loss = 0\n    total_ce_loss = 0\n    total_att_loss = 0\n    \n    progress_bar = tqdm(train_loader, desc=\"Training\")\n    \n    for images, captions, lengths in progress_bar:\n        images = images.to(device)\n        captions = captions.to(device)\n        lengths = lengths.to(device)\n        \n        encoder_out = encoder(images)\n        predictions, alphas, sorted_captions, decode_lengths, sort_idx = decoder(\n            encoder_out, captions, lengths\n        )\n        \n        targets = sorted_captions[:, 1:]\n        \n        predictions_packed = torch.cat([\n            predictions[i, :decode_lengths[i], :] \n            for i in range(len(decode_lengths))\n        ], dim=0)\n        \n        targets_packed = torch.cat([\n            targets[i, :decode_lengths[i]] \n            for i in range(len(decode_lengths))\n        ], dim=0)\n        \n        ce_loss = criterion(predictions_packed, targets_packed)\n        \n        alphas_packed = torch.cat([\n            alphas[i, :decode_lengths[i], :] \n            for i in range(len(decode_lengths))\n        ], dim=0)\n        \n        att_regularization = alpha_c * ((1 - alphas_packed.sum(dim=0)) ** 2).mean()\n        loss = ce_loss + att_regularization\n        \n        decoder_optimizer.zero_grad()\n        if encoder_optimizer is not None:\n            encoder_optimizer.zero_grad()\n        \n        loss.backward()\n        \n        torch.nn.utils.clip_grad_norm_(decoder.parameters(), max_norm=5.0)\n        if encoder_optimizer is not None:\n            torch.nn.utils.clip_grad_norm_(encoder.parameters(), max_norm=5.0)\n        \n        decoder_optimizer.step()\n        if encoder_optimizer is not None:\n            encoder_optimizer.step()\n        \n        total_loss += loss.item()\n        total_ce_loss += ce_loss.item()\n        total_att_loss += att_regularization.item()\n        progress_bar.set_postfix({'loss': f'{ce_loss.item():.4f}'})\n    \n    return total_ce_loss / len(train_loader), total_att_loss / len(train_loader)\n\ndef validate(encoder, decoder, val_loader, criterion, device):\n    encoder.eval()\n    decoder.eval()\n    \n    total_loss = 0\n    \n    with torch.no_grad():\n        for images, captions, lengths in tqdm(val_loader, desc=\"Validating\"):\n            images = images.to(device)\n            captions = captions.to(device)\n            lengths = lengths.to(device)\n            \n            encoder_out = encoder(images)\n            predictions, alphas, sorted_captions, decode_lengths, sort_idx = decoder(\n                encoder_out, captions, lengths\n            )\n            \n            targets = sorted_captions[:, 1:]\n            \n            predictions_packed = torch.cat([\n                predictions[i, :decode_lengths[i], :] \n                for i in range(len(decode_lengths))\n            ], dim=0)\n            \n            targets_packed = torch.cat([\n                targets[i, :decode_lengths[i]] \n                for i in range(len(decode_lengths))\n            ], dim=0)\n            \n            loss = criterion(predictions_packed, targets_packed)\n            total_loss += loss.item()\n    \n    return total_loss / len(val_loader)\n\ndef save_checkpoint(epoch, encoder, decoder, encoder_optimizer, decoder_optimizer, \n                   train_loss, val_loss, vocab_size, checkpoint_path):\n    checkpoint = {\n        'epoch': epoch,\n        'encoder_state_dict': encoder.state_dict(),\n        'decoder_state_dict': decoder.state_dict(),\n        'decoder_optimizer_state_dict': decoder_optimizer.state_dict(),\n        'encoder_optimizer_state_dict': encoder_optimizer.state_dict() if encoder_optimizer else None,\n        'train_loss': train_loss,\n        'val_loss': val_loss,\n        'vocab_size': vocab_size,\n        'embed_dim': EMBED_DIM,\n        'hidden_dim': HIDDEN_DIM,\n        'attention_dim': ATTENTION_DIM,\n        'encoder_dim': ENCODER_DIM,\n        'dropout': DROPOUT\n    }\n    torch.save(checkpoint, checkpoint_path)",
   "id": "2094b1486e48af15"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-23T09:08:39.563445680Z",
     "start_time": "2026-01-22T12:12:32.322192Z"
    },
    "execution": {
     "iopub.execute_input": "2026-01-23T00:31:23.080408Z",
     "iopub.status.busy": "2026-01-23T00:31:23.080135Z",
     "iopub.status.idle": "2026-01-23T00:32:28.577312Z",
     "shell.execute_reply": "2026-01-23T00:32:28.576484Z"
    },
    "papermill": {
     "duration": 65.517566,
     "end_time": "2026-01-23T00:32:28.588002",
     "exception": false,
     "start_time": "2026-01-23T00:31:23.070436",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": "if os.path.exists(CAPTIONS_FILE):\n    encoder = EncoderCNN(fine_tune=False).to(device)\n    \n    decoder = DecoderLSTM(\n        embed_dim=EMBED_DIM,\n        decoder_dim=HIDDEN_DIM,\n        attention_dim=ATTENTION_DIM,\n        vocab_size=len(vocab),\n        encoder_dim=ENCODER_DIM,\n        dropout=DROPOUT\n    ).to(device)\n    \n    criterion = nn.CrossEntropyLoss(ignore_index=vocab.word2idx[vocab.PAD_TOKEN])\n    \n    decoder_optimizer = optim.Adam(decoder.parameters(), lr=LEARNING_RATE)\n    encoder_optimizer = None\n    \n    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n        decoder_optimizer, mode='min', factor=0.5, patience=3\n    )\n    \n    history = {\n        'train_loss': [],\n        'val_loss': []\n    }\n    \n    best_val_loss = float('inf')\n\n    print(f\"Starting training for {NUM_EPOCHS} epochs\")\n    print(f\"Encoder params: {sum(p.numel() for p in encoder.parameters()):,}\")\n    print(f\"Decoder params: {sum(p.numel() for p in decoder.parameters()):,}\")\nelse:\n    print(\"Dataset not found. Please extract caption_data.zip first.\")",
   "id": "5e0d5b26d395e315"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-23T09:08:39.563933533Z",
     "start_time": "2026-01-22T12:12:32.817848Z"
    },
    "execution": {
     "iopub.execute_input": "2026-01-23T00:32:28.607397Z",
     "iopub.status.busy": "2026-01-23T00:32:28.606979Z",
     "iopub.status.idle": "2026-01-23T02:09:24.792219Z",
     "shell.execute_reply": "2026-01-23T02:09:24.791390Z"
    },
    "papermill": {
     "duration": 5816.198323,
     "end_time": "2026-01-23T02:09:24.794712",
     "exception": false,
     "start_time": "2026-01-23T00:32:28.596389",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": "if os.path.exists(CAPTIONS_FILE):\n    best_model_path = os.path.join(SAVE_DIR, 'best_model.pt')\n    \n    for epoch in range(NUM_EPOCHS):\n        print(f\"\\nEpoch {epoch + 1}/{NUM_EPOCHS}\")\n        \n        if FINE_TUNE_ENCODER and epoch == FINE_TUNE_AFTER:\n            print(\"Starting encoder fine-tuning\")\n            encoder.set_fine_tuning(True)\n            encoder_optimizer = optim.Adam(\n                filter(lambda p: p.requires_grad, encoder.parameters()),\n                lr=ENCODER_LR\n            )\n        \n        train_ce_loss, train_att_loss = train_one_epoch(\n            encoder, decoder, train_loader, criterion,\n            decoder_optimizer, encoder_optimizer, device\n        )\n        \n        val_loss = validate(encoder, decoder, val_loader, criterion, device)\n        \n        scheduler.step(val_loss)\n        \n        history['train_loss'].append(train_ce_loss)\n        history['val_loss'].append(val_loss)\n        \n        print(f\"\\nEpoch {epoch + 1} Summary:\")\n        print(f\"  Train Loss (CE): {train_ce_loss:.4f}\")\n        print(f\"  Train Loss (Att Reg): {train_att_loss:.4f}\")\n        print(f\"  Val Loss: {val_loss:.4f}\")\n        \n        if val_loss < best_val_loss:\n            best_val_loss = val_loss\n            save_checkpoint(\n                epoch, encoder, decoder, encoder_optimizer, decoder_optimizer,\n                train_ce_loss, val_loss, len(vocab),\n                best_model_path\n            )\n            print(f\"  Best model saved (val loss: {val_loss:.4f})\")\n        else:\n            print(f\"  Not improved (best: {best_val_loss:.4f})\")\n    \n    print(\"\\nTraining complete!\")\n    print(f\"Best validation loss: {best_val_loss:.4f}\")\n    print(f\"Model saved to: {best_model_path}\")\nelse:\n    print(\"Dataset not found. Skipping training.\")",
   "id": "e3c3bbe18eff40fd"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(CAPTIONS_FILE) and len(history['train_loss']) > 0:\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history['train_loss'], label='Train Loss', marker='o')\n",
    "    plt.plot(history['val_loss'], label='Val Loss', marker='s')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    if FINE_TUNE_ENCODER and FINE_TUNE_AFTER < len(history['train_loss']):\n",
    "        plt.axvline(x=FINE_TUNE_AFTER, color='r', linestyle='--', \n",
    "                   label=f'Fine-tune start (epoch {FINE_TUNE_AFTER})')\n",
    "        plt.legend()\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    improvements = [0] + [history['val_loss'][i-1] - history['val_loss'][i] \n",
    "                         for i in range(1, len(history['val_loss']))]\n",
    "    colors = ['green' if imp > 0 else 'red' for imp in improvements]\n",
    "    plt.bar(range(len(improvements)), improvements, color=colors)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss Improvement')\n",
    "    plt.title('Validation Loss Improvement per Epoch')\n",
    "    plt.axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
    "    plt.grid(True, axis='y')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(SAVE_DIR, 'training_history.png'), dpi=150)\n",
    "    plt.show()\n",
    "    \n",
    "    with open(os.path.join(SAVE_DIR, 'training_history.json'), 'w') as f:\n",
    "        json.dump(history, f, indent=2)\n",
    "    print(\"Training history saved\")"
   ],
   "id": "e0bf8d648dc72cc5"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.653359,
     "end_time": "2026-01-23T02:09:28.052092",
     "exception": false,
     "start_time": "2026-01-23T02:09:27.398733",
     "status": "completed"
    },
    "tags": []
   },
   "source": "## Sample Predictions\n\nVisualize generated captions on validation images to verify model performance.",
   "id": "69a7791a487bef53"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-23T02:09:29.431878Z",
     "iopub.status.busy": "2026-01-23T02:09:29.431615Z",
     "iopub.status.idle": "2026-01-23T02:09:34.082164Z",
     "shell.execute_reply": "2026-01-23T02:09:34.081258Z"
    },
    "papermill": {
     "duration": 5.319421,
     "end_time": "2026-01-23T02:09:34.102449",
     "exception": false,
     "start_time": "2026-01-23T02:09:28.783028",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": "if os.path.exists(CAPTIONS_FILE):\n    encoder.eval()\n    decoder.eval()\n    \n    sample_images, sample_captions, sample_lengths = next(iter(val_loader))\n    \n    def denormalize(tensor):\n        mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n        std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n        return tensor * std + mean\n    \n    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n    axes = axes.flatten()\n    \n    with torch.no_grad():\n        for i in range(min(6, len(sample_images))):\n            img = sample_images[i:i+1].to(device)\n            \n            encoder_out = encoder(img)\n            generated_caption, _ = decoder.generate(encoder_out, vocab, max_len=30, beam_size=3)\n            \n            gt_caption = vocab.decode(sample_captions[i].tolist())\n            \n            img_display = denormalize(sample_images[i]).permute(1, 2, 0).numpy()\n            img_display = np.clip(img_display, 0, 1)\n            axes[i].imshow(img_display)\n            axes[i].axis('off')\n            axes[i].set_title(f\"Generated: {generated_caption[:50]}...\\n\"\n                             f\"Ground Truth: {gt_caption[:50]}...\", \n                             fontsize=8, wrap=True)\n    \n    plt.tight_layout()\n    plt.savefig(os.path.join(SAVE_DIR, 'sample_predictions.png'), dpi=150)\n    plt.show()\n    print(\"Sample predictions saved\")",
   "id": "5c594e4b1d6d48d6"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.659849,
     "end_time": "2026-01-23T02:09:35.506076",
     "exception": false,
     "start_time": "2026-01-23T02:09:34.846227",
     "status": "completed"
    },
    "tags": []
   },
   "source": "## Model Evaluation\n\nBLEU score evaluation on test set. BLEU measures n-gram precision between generated and reference captions (BLEU-1 through BLEU-4).",
   "id": "982f8c567839bc49"
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-23T02:09:36.949209Z",
     "iopub.status.busy": "2026-01-23T02:09:36.948831Z",
     "iopub.status.idle": "2026-01-23T02:09:36.958039Z",
     "shell.execute_reply": "2026-01-23T02:09:36.957304Z"
    },
    "papermill": {
     "duration": 0.699748,
     "end_time": "2026-01-23T02:09:36.959568",
     "exception": false,
     "start_time": "2026-01-23T02:09:36.259820",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# BLEU SCORE EVALUATION\n",
    "# ============================================\n",
    "\n",
    "def evaluate_bleu(encoder, decoder, test_captions, images_dir, vocab, device, \n",
    "                  transform, num_samples=None):\n",
    "    \"\"\"\n",
    "    Evaluate model using BLEU scores on test set.\n",
    "    \n",
    "    Args:\n",
    "        encoder: Trained CNN encoder\n",
    "        decoder: Trained LSTM decoder\n",
    "        test_captions: Dict of {image_name: [reference_captions]}\n",
    "        images_dir: Path to images directory\n",
    "        vocab: Vocabulary object\n",
    "        device: torch device\n",
    "        transform: Image transform\n",
    "        num_samples: Number of samples to evaluate (None = all)\n",
    "    \n",
    "    Returns:\n",
    "        dict: BLEU-1, BLEU-2, BLEU-3, BLEU-4 scores\n",
    "    \"\"\"\n",
    "    if not BLEU_AVAILABLE:\n",
    "        print(\"NLTK not available. Skipping BLEU evaluation.\")\n",
    "        return None\n",
    "    \n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    \n",
    "    references = []  # List of list of reference captions (tokenized)\n",
    "    hypotheses = []  # List of generated captions (tokenized)\n",
    "    \n",
    "    image_names = list(test_captions.keys())\n",
    "    if num_samples:\n",
    "        image_names = image_names[:num_samples]\n",
    "    \n",
    "    print(f\"Evaluating BLEU on {len(image_names)} images...\")\n",
    "    \n",
    "    smoothing = SmoothingFunction().method1\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for img_name in tqdm(image_names, desc=\"Generating captions\"):\n",
    "            # Load and preprocess image\n",
    "            img_path = os.path.join(images_dir, img_name)\n",
    "            if not os.path.exists(img_path):\n",
    "                continue\n",
    "                \n",
    "            image = Image.open(img_path).convert('RGB')\n",
    "            image_tensor = transform(image).unsqueeze(0).to(device)\n",
    "            \n",
    "            # Generate caption\n",
    "            encoder_out = encoder(image_tensor)\n",
    "            generated_caption, _ = decoder.generate(encoder_out, vocab, max_len=30, beam_size=3)\n",
    "            \n",
    "            # Tokenize generated caption\n",
    "            hypothesis = generated_caption.split()\n",
    "            \n",
    "            # Get reference captions (all 5 per image)\n",
    "            refs = [cap.split() for cap in test_captions[img_name]]\n",
    "            \n",
    "            references.append(refs)\n",
    "            hypotheses.append(hypothesis)\n",
    "    \n",
    "    # Calculate BLEU scores\n",
    "    bleu1 = corpus_bleu(references, hypotheses, weights=(1, 0, 0, 0))\n",
    "    bleu2 = corpus_bleu(references, hypotheses, weights=(0.5, 0.5, 0, 0))\n",
    "    bleu3 = corpus_bleu(references, hypotheses, weights=(0.33, 0.33, 0.33, 0))\n",
    "    bleu4 = corpus_bleu(references, hypotheses, weights=(0.25, 0.25, 0.25, 0.25))\n",
    "    \n",
    "    results = {\n",
    "        'BLEU-1': bleu1,\n",
    "        'BLEU-2': bleu2,\n",
    "        'BLEU-3': bleu3,\n",
    "        'BLEU-4': bleu4\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(\"BLEU SCORE RESULTS\")\n",
    "    print(f\"{'='*50}\")\n",
    "    print(f\"BLEU-1: {bleu1:.4f}\")\n",
    "    print(f\"BLEU-2: {bleu2:.4f}\")\n",
    "    print(f\"BLEU-3: {bleu3:.4f}\")\n",
    "    print(f\"BLEU-4: {bleu4:.4f}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    return results"
   ],
   "id": "4c76bf0a351a6c40"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-23T02:09:38.379272Z",
     "iopub.status.busy": "2026-01-23T02:09:38.378579Z",
     "iopub.status.idle": "2026-01-23T02:09:42.440249Z",
     "shell.execute_reply": "2026-01-23T02:09:42.439525Z"
    },
    "papermill": {
     "duration": 4.821316,
     "end_time": "2026-01-23T02:09:42.441876",
     "exception": false,
     "start_time": "2026-01-23T02:09:37.620560",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": "if os.path.exists(CAPTIONS_FILE):\n    bleu_results = evaluate_bleu(\n        encoder, decoder, test_captions, IMAGES_DIR, vocab, device,\n        val_transform, num_samples=100\n    )\n    \n    if bleu_results:\n        with open(os.path.join(SAVE_DIR, 'bleu_scores.json'), 'w') as f:\n            json.dump(bleu_results, f, indent=2)\n        \n        plt.figure(figsize=(8, 5))\n        bleu_names = list(bleu_results.keys())\n        bleu_values = list(bleu_results.values())\n        \n        bars = plt.bar(bleu_names, bleu_values, color=['#3498db', '#2ecc71', '#f39c12', '#e74c3c'])\n        plt.ylabel('Score')\n        plt.title('BLEU Scores on Test Set')\n        plt.ylim(0, 1)\n        \n        for bar, val in zip(bars, bleu_values):\n            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02, \n                    f'{val:.3f}', ha='center', va='bottom', fontsize=11)\n        \n        plt.grid(True, axis='y', alpha=0.3)\n        plt.tight_layout()\n        plt.savefig(os.path.join(SAVE_DIR, 'bleu_scores.png'), dpi=150)\n        plt.show()\n        print(\"BLEU scores saved\")",
   "id": "2564f054f7157f65"
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 9314753,
     "sourceId": 14581955,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31259,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 5924.492825,
   "end_time": "2026-01-23T02:09:47.345295",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2026-01-23T00:31:02.852470",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
